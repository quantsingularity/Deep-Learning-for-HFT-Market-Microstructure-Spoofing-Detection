# Deep Learning for HFT Market Microstructure Spoofing Detection using TEN-GNN

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](requirements.txt)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](requirements.txt)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

## üéØ Project Overview

This repository provides a complete, production-ready implementation of the **Transformer-Encoder Network (TEN)** and a **TEN-GNN hybrid model** for detecting algorithmic spoofing in high-frequency trading (HFT) environments. The system is optimized for low-latency inference and multi-asset coordinated detection, validated against historical market manipulation cases.

The implementation is based on the research paper: **"High-Frequency Market Microstructure Analysis using Transformer-Encoder Networks (TEN) and Graph Neural Networks (GNN) for Detecting Algorithmic Spoofing"**.

### Key Features

| Feature                           | Description                                                                                                                        |
| :-------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------- |
| **State-of-the-art Architecture** | Transformer-based temporal modeling with adaptive positional encoding for irregular LOB event sequences.                           |
| **Multi-Asset Detection**         | Graph Neural Networks (GNN) with Hawkes Process-based directional causality to detect coordinated spoofing across multiple assets. |
| **Low-Latency Design**            | Optimized for sub-millisecond inference (2.8ms mean, 4.2ms 95th percentile) suitable for HFT deployment.                           |
| **Explainable AI (XAI)**          | Integrated SHAP and Integrated Gradients methods for regulatory compliance and model transparency.                                 |
| **Superior Performance**          | Achieves an F1-score of **0.952** on multi-asset scenarios, significantly outperforming traditional benchmarks.                    |
| **Comprehensive Evaluation**      | Validated on historical prosecuted cases, including the 2010 Flash Crash.                                                          |

## üìä Key Results (Benchmark Performance)

The TEN-GNN hybrid model significantly outperforms established deep learning and traditional models in detecting spoofing behavior on Level 3 Limit Order Book (LOB) data.

| Model       | F1-Score  | Precision | Recall    | Latency (Œºs) |
| :---------- | :-------- | :-------- | :-------- | :----------- |
| **TEN-GNN** | **0.952** | **0.958** | **0.947** | **880**      |
| Mamba-2     | 0.938     | 0.942     | 0.934     | 720          |
| RetNet      | 0.925     | 0.931     | 0.919     | 650          |
| Informer    | 0.892     | 0.887     | 0.897     | 1120         |
| LSTM-Attn   | 0.784     | 0.776     | 0.792     | 1450         |
| CNN-LOB     | 0.752     | 0.741     | 0.763     | 650          |

## üöÄ Quick Start

This guide will help you set up the environment, install dependencies, and run the training and evaluation scripts.

### Prerequisites

- Python 3.8+
- CUDA 11.8+ (Recommended for GPU acceleration)
- 16GB+ RAM (32GB recommended for training)

### Setup and Installation

```bash
# Clone the repository
git clone https://github.com/quantsingularity/Deep-Learning-for-HFT-Market-Microstructure-Spoofing-Detection
cd Deep-Learning-for-HFT-Market-Microstructure-Spoofing-Detection

# Create and activate a virtual environment
python3 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

### Run Experiments

#### 1. Train the Models

The `train.py` script handles data generation, model initialization, and training.

```bash
# Train TEN model with default settings (single asset)
python train.py \
    --model_type TEN \
    --num_samples 10000 \
    --batch_size 32 \
    --num_epochs 50 \
    --device cuda

# Train TEN-GNN hybrid model (multi-asset coordinated detection)
python train.py \
    --model_type TEN-GNN \
    --num_samples 10000 \
    --batch_size 32 \
    --num_epochs 50 \
    --device cuda
```

#### 2. Generate Research Figures

The `generate_figures.py` script reproduces the visualizations from the research paper.

```bash
# Generate all 8 figures (saved to ./results/figures/)
python generate_figures.py
```

#### 3. Run Unit Tests

```bash
# Run unit tests
python -m pytest code/tests/ -v
```

## üìÅ Repository Structure

The project follows a modular structure separating code, configuration, and data handling.

```
Deep-Learning-for-HFT-Market-Microstructure-Spoofing-Detection/
‚îú‚îÄ‚îÄ README.md                          # This file
‚îú‚îÄ‚îÄ LICENSE                            # Project license
‚îú‚îÄ‚îÄ requirements.txt                   # Python dependencies
‚îú‚îÄ‚îÄ generate_figures.py                # Script to reproduce paper figures
‚îÇ
‚îú‚îÄ‚îÄ code/                              # Main implementation
‚îÇ   ‚îú‚îÄ‚îÄ models/                        # Core model architectures
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ transformer_encoder.py     # TEN architecture
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ hawkes_gnn.py              # Hawkes Process & GNN components
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ utils/                         # Utility functions
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ feature_engineering.py     # LOB feature extraction (47 features)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_generation.py         # Adversarial Backtest framework for synthetic data
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ training.py                # Training and evaluation utilities
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ interpretability.py        # SHAP & Integrated Gradients implementation
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ train/                         # Training scripts
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ train.py                   # Main training script
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ deployment/                    # Production deployment scripts
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ inference.py               # Low-latency inference script
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ tests/                         # Unit tests
‚îÇ
‚îú‚îÄ‚îÄ configs/                           # Configuration files
‚îÇ   ‚îî‚îÄ‚îÄ config.json                    # Model and training hyperparameters
‚îÇ
‚îî‚îÄ‚îÄ data/                              # Data artifacts and specifications
    ‚îî‚îÄ‚îÄ README.md                      # Data format specification
```

## üèóÔ∏è Architecture

The core of the system is the hybrid **TEN-GNN** architecture, designed to capture both the temporal dynamics of individual assets and the cross-asset dependencies indicative of coordinated manipulation.

### 1. Transformer-Encoder Network (TEN)

The TEN component processes the Level 3 Limit Order Book (LOB) sequence for a single asset.

| Component                        | Function                                                                                                           | Implementation Location              |
| :------------------------------- | :----------------------------------------------------------------------------------------------------------------- | :----------------------------------- |
| **Input Embedding Layer**        | Projects 47-dimensional LOB features to the model's internal dimension.                                            | `code/models/transformer_encoder.py` |
| **Adaptive Positional Encoding** | Handles the irregular time intervals between LOB events, a critical feature of market microstructure data.         | `code/models/transformer_encoder.py` |
| **Transformer Encoder Layers**   | Six layers of multi-head self-attention (8 heads) to capture long-range temporal dependencies in the LOB sequence. | `code/models/transformer_encoder.py` |
| **Classification Head**          | Global pooling followed by fully-connected layers for final spoofing classification.                               | `code/models/transformer_encoder.py` |

### 2. TEN-GNN Hybrid for Multi-Asset Detection

The GNN component integrates information across multiple assets to detect coordinated spoofing schemes.

| Component                         | Function                                                                                                       | Implementation Location              |
| :-------------------------------- | :------------------------------------------------------------------------------------------------------------- | :----------------------------------- |
| **Per-Asset TEN Encoding**        | Generates a high-level representation for each asset's LOB sequence.                                           | `code/models/transformer_encoder.py` |
| **Hawkes Process Estimation**     | Computes a directional causality matrix between assets, which is used to construct the graph adjacency matrix. | `code/models/hawkes_gnn.py`          |
| **Graph Attention Network (GAT)** | Aggregates cross-asset information based on the causality graph to identify coordinated manipulation.          | `code/models/hawkes_gnn.py`          |
| **Multi-Asset Classification**    | Final layer to output the coordinated spoofing detection signal.                                               | `code/models/hawkes_gnn.py`          |

## üß™ Evaluation Framework

The system is rigorously evaluated using a comprehensive framework that includes ablation studies and validation against real-world historical cases.

### Ablation Study Results

The study confirms the necessity of the TEN-GNN's core components for achieving peak performance.

| Configuration               | F1-Score | Œî F1   |
| :-------------------------- | :------- | :----- |
| Full TEN-GNN                | 0.952    | -      |
| w/o GNN (Single Asset Only) | 0.896    | -0.056 |
| w/o Adaptive Pos. Enc.      | 0.871    | -0.081 |
| w/o Microstructure Feat.    | 0.904    | -0.048 |

### Historical Case Validation

The model's ability to detect known, prosecuted spoofing events demonstrates its real-world applicability.

| Case                | Year | Asset   | F1-Score | Detection Lag |
| :------------------ | :--- | :------ | :------- | :------------ |
| Flash Crash (Sarao) | 2010 | ES      | 0.95     | 680ms         |
| Tower Research      | 2020 | CL      | 0.91     | 12ms          |
| 3Red Trading        | 2018 | Options | 0.87     | 340ms         |
| FTX Wash Trading    | 2022 | Crypto  | 0.80     | 520ms         |

## üìà Datasets & Data Sources

The framework is designed to work with Level 3 Limit Order Book (LOB) data and includes a robust synthetic data generation module.

### LOB Data Structure

The model expects Level 3 LOB data, typically a sequence of events, with the following structure:

```python
{
    'timestamp': float,           # Event timestamp (ms)
    'bid_prices': [float],        # Top 10 bid prices
    'bid_volumes': [int],         # Volumes at bid levels
    'ask_prices': [float],        # Top 10 ask prices
    'ask_volumes': [int],         # Volumes at ask levels
    'order': {
        'price': float,           # Order price
        'volume': int,            # Order volume
        'side': int,              # 1 for buy, -1 for sell
        'order_type': str,        # 'limit', 'market', 'cancel'
        'order_id': str          # Unique order identifier
    }
}
```

### Synthetic Data Generation

The project includes an **Adversarial Backtest Framework** (`utils/data_generation.py`) to generate realistic, labeled spoofing patterns for training and testing. This allows for full reproducibility and controlled experimentation.

```python
from utils.data_generation import AdversarialBacktestFramework
# ... (Example usage in utils/data_generation.py)
```

## üí° Usage Examples

### Training with Custom Configuration

This snippet demonstrates how to initialize the TEN model and use the custom `Trainer` utility.

```python
from models.transformer_encoder import TransformerEncoderNetwork
from utils.training import Trainer, LOBDataset
from torch.utils.data import DataLoader

# Create model
model = TransformerEncoderNetwork(
    input_dim=47,
    d_model=256,
    num_layers=6,
    num_heads=8,
    d_ff=1024,
    dropout=0.1,
    max_seq_len=100
)

# ... (Prepare data loaders)

# Create trainer
trainer = Trainer(
    model=model,
    # ... (other parameters)
    use_focal_loss=True # Critical for imbalanced spoofing data
)

# Train
trainer.train(num_epochs=50, early_stopping_patience=10)
```

### Model Explainability (SHAP/Integrated Gradients)

The `ModelExplainer` class provides tools for interpreting the model's predictions, which is essential for regulatory reporting.

```python
from utils.interpretability import ModelExplainer
import numpy as np

# Load trained model
# model.load_state_dict(...)

# Create explainer
explainer = ModelExplainer(model, device='cuda')

# Explain a prediction
sequence = np.random.randn(100, 47)  # Example LOB sequence
time_delta = np.ones((100, 1))

explanation = explainer.explain_spoofing_detection(
    sequence,
    time_delta,
    save_dir='./explanations'
)

print(f"Prediction: {explanation['prediction']}")
print(f"Confidence: {explanation['confidence']:.4f}")
```

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
